# (1) why cost function
在 Rosenblatt Perceptron 的训练过程中，输入一个样本就更新一次权重集合 θ 。每个 iris 样本输入进去，就会得到输出，然后更新权重。样本集共有n个样本，模型会更新n次，再加上外层设置的迭代次数m，模型一共被更新m*n次。然而，最后得到的模型并不一定是最优的结果。学习过程并不会自己找到最优的结果并停止。它之所以停止是因为认为设置了迭代次数。
于是设想，构造一个关于模型 θ 的函数 f(θ) ，当 f(θ) 收敛时，分类结果是最优的，那么此时的 θ 就是最优的模型。只要这个函数存在，程序就能自己找到最优模型。那么在学习过程中是否存在一个这样的具有收敛性的函数？

# (2) prove existence
Frank Rosenblatt从数学上证明了只要两个类别能够被一个线性超平面分开，则感知机算法一定能够收敛。
```
线性超平面是一个几何体，可以用方程表示，
超平面的自由度比所在n维空间低一个维度，即给定n-1个轴的坐标，可以确定剩下一个轴的坐标。
方程是线性的: 是空间点的各分量的线性组合
方程数量为1
在三维坐标系中，三个轴坐标值的线性组合确定一个超平面即平面
                Ax+By+Cz+D=0，A,B,C,D为常数
在平面直角坐标系中，超平面即直线
                Ax+By+C=0
推广到n维空间，就统一叫做超平面。
```
模型就是为了分类，而分类就是结果，既然算法能有效分类，按照前面的理论，这个算法自然就会收敛。<br>
两个坐标集可以被坐标系中的超平面分开。
而对于两个类别，可以使用类别的特征建立坐标系，在这个坐标系里寻找超平面分割两个类别，那么就会产生超平面方程。
超平面方程，感知机算法（模型的分类算法），假想函数 f(θ)，cost/losss函数，这四者有什么关联呢？
# (3) 构造cost function
上边提到模型的分类算法，它具有收敛性。
但是还存在超平面方程， f(θ), cost/loss function。要先理清它们的关系。

在Perceptron里所有用到的公式如下

激活值

$$z^{(i)} = \vec {w}^T\vec x^{(i)}+b$$

激活函数

$$
\phi(z) = 
{\begin{cases}
\ \ \ 1\ ,\ z>0\\
-1\ ,\ others
\end{cases}}
$$

更新模型

$$
w_j=w_j+\Delta w_j 
$$
<br/>

$$
\Delta w_j=\eta (y^{(i)} - \hat y^{(i)})x^{(i)}_j
$$

```
以下这个公式是Perceptron算法的关键
```

$$y^{(i)} - \hat y^{(i)}$$
```
Perceptron的预测结果和标签只有-1和1，算法的关键在于权重更新方程中，期望值减预测结果。权重对输入起到正激励（激励）或负激励（抑制）作用，预测错误即权重起了相反的作用，那么原本起激励作用的权重应做相应的减小以增加抑制倾向（-1-1=-2，Δw<0，w减小，wx减小），原本起抑制作用的权重应做相应的增大以增加激励倾向（1--1=2，Δw>0，w增大，wx增大），所以这个操作使下次计算结果朝着正确的方向前进。
```

那么这个模型的分类算法——收敛性的算法，和上边三个公式有什么关系？<br>
cost function又在哪？该怎么构造？<br>
### **先看下```Adaline```的损失函数**

$$\bold J(\bold w)=\frac {1}{2}\sum_i(y^{(i)}-\phi(z^{(i)}))^2$$

```
在Rosenblatt Perceptron中，每输入一个样本就会更新一次模型，它的损失函数可以理解为
```
$$\bold J(\bold w)=y^{(i)}-\phi(z^{(i)})$$
```
在这个公式的好处是可以自行调整
```
在Adaline中，一次迭代更新一次模型，而不是一个样本更新一次。<br>
收集每个样本预测的误差，一个好的方式是收集和，但是可能存在负数，为了保证不在相加的过程中互相叠加。期望值与预测结果差的平方和







